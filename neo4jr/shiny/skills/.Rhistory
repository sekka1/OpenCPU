runApp
topSchoolsByVcs()
source('~/git/opencpu/neo4j-r/R/do.R')
source('~/git/opencpu/neo4j-r/R/do.R')
topSchoolsByFounders()
students <- topSchoolsByVcs
students <- topSchoolsByVcs()
students <- topSchoolsByFounders()
students
head(students)
source('~/git/opencpu/neo4j-r/R/do.R')
topSchoolsByFounders()
source('~/git/opencpu/neo4j-r/R/do.R')
topSchoolsByFounders()
source('~/git/opencpu/neo4j-r/R/do.R')
topSchoolsByFounders()
source('~/git/opencpu/neo4j-r/R/do.R')
ls()
sapply(ls(), object.size)
sort(sapply(ls(), object.size))
sort(sapply(ls(), object.size), decreasing=T)
debug(queryCypher2)
source('~/git/opencpu/neo4jr/R/do.R')
queryCypher2("match s:Skill return s.name!, s.display_name! limit 10")
source('~/git/opencpu/neo4jr/R/do.R')
queryCypher2("match s:Skill return s.name!, s.display_name! limit 10")
debug(queryCypher2)
queryCypher2("match s:Skill return s.name!, s.display_name! limit 10")
d
class(d)
t(d)
data
data.frame(t(d))
is.list(d)
class(d)
is.matrix(d)
Q
queryCypher2("match s:Skill return s.name! limit 10")
d
class(d)
is.matrix(d)
source('~/git/opencpu/neo4jr/R/do.R')
Q
source('~/git/opencpu/neo4jr/R/do.R')
queryCypher2("match s:Skill return s.name! limit 10")
queryCypher2("match s:Skill return s.name!, s.display_name! limit 10")
source('~/git/opencpu/neo4jr/R/do.R')
queryCypher2("match s:Skill return s.display_name! limit 10")
queryCypher2("match s:Skill return collect(s.display_name!)")
?Corpus
install.packages("tm")
library("tm")
Corpus("hello world")
?Corpus
?DirSource
skills
queryCypher2("match s:Skill return s.display_name!")
queryCypher2("match s:Skill return s.name!")
queryCypher2("match s:Skill return s.display_name! limit 10")
queryCypher2("match s:Skill return s.display_name!")
skills <- queryCypher2("match s:Skill return s.display_name!")
head(skills)
DataframeSource(skills)
corpus <- Corpus(x=DataframeSource(skills))
tdm <- TermDocumentMatrix(corpus)
as.matrix(tdm)
head(as.matrix(tdm))
as.matrix(tdm)[1,1]
as.matrix(tdm)[1,2]
names(as.matrix(tdm))
library(wordcloud)
m <- as.matrix(tdm)
sort(rowSums(m), decreasing=TRUE)
head(sort(rowSums(m), decreasing=TRUE))
head(sort(rowSums(m), decreasing=TRUE),40)
?TermDocumentMatric
?TermDocumentMatrix
dtm <- DocumentTermMatrix(corpus,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
FALSE),
stopwords = TRUE))
m <- as.matrix(tdm)
head(sort(rowSums(m), decreasing=TRUE),40)
?stopwords("english")
stopwords("english")
?tm_map
?removeWords
corpus <- tm_map(x=corpus, FUN=removeWords, stopwords("english"))
m <- as.matrix(tdm)
head(sort(rowSums(m), decreasing=TRUE),40)
grepl("and", stopwords("english"))
corpus <- tm_map(x=corpus, FUN=removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
head(sort(rowSums(m), decreasing=TRUE),20)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
set.seed(375)
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F, colors=grayLevels)
library("RWeka")
install.packages("RWeka")
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
head(corpus)
corpus
head(skills)
head(tail(skills))
head(tail(skills, -1))
corpus <- tm_map(x=DataframeSource(tail(skills,-1)), FUN=removeWords, stopwords("english"))
corpus <- tm_map(x=DataframeSource(skills), FUN=removeWords, stopwords("english"))
corpus <- tm_map(x=Corpus(DataframeSource(tail(skills,-1))), FUN=removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
BigramTokenizer
?rep
debug(TermDocumentMatrix)
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
head(v)
head(tflist)
head(termFreq)
control
head(x)
Q
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
tflist
head(tflist)
?
hello
l
list
install.packages("rmr")
Q
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
x
termFreq
head(tflist)
TermDocumentMatrix
Q
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
Q
?Weka_control
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(max = 2))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
head(tflist)
?termFreq
?parallel::mcapply
?parallel::mclapply
Q
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
parallel::mclapply(corpus, FUN=print, control = list(tokenize = BigramTokenizer))
BigramTokenizer
BigramTokenizer("component specification")
BigramTokenizer("component specification kjfdkj fdhj")
class(BigramTokenizer("component specification"))
parallel::mclapply(corpus, FUN=termFreq)
?termFreq
?NGramTokenizer
parallel::mclapply(corpus, FUN=print, control = list(tokenize = NGramTokenizer))
parallel::mclapply(corpus, FUN=print, control = list(tokenize = function(x) NGramTokenizer(x, Weka_control(min=2))))
parallel::mclapply(corpus, FUN=print, control = list(tokenize = function(x) NGramTokenizer(x, Weka_control(max=2))))
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) NGramTokenizer(x, Weka_control(max=2))))
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) NGramTokenizer(x, Weka_control(min=2))))
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) NGramTokenizer(x, Weka_contr
parallel::mclapply(corpus, FUN=print, control = list(tokenize = NGramTokenizer))
parallel::mclapply(corpus, FUN=print, control = list(tokenize = NGramTokenizer))
parallel::mclapply(corpus, FUN=termFreq
);
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) NGramTokenizer(x)))
parallel::mclapply(corpus, FUN=class)
?NGramTokenizer
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) NGramTokenizer(as.charater(x))))
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) NGramTokenizer(as.charater(x?
;
?as.character
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) NGramTokenizer(charater(x?
)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) NGramTokenizer(character(x)))
)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) print(class(x))))
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) print("who am i"))
)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = function(x) c("hello", "world")))
?termFreq
?scan_tokenizer
?NGramTokenizer
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = NGramTokenizer))
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = scan_tokenizer))
corpus[7623]
corpus$`7623`
corpus$7623
corpus[7623]
scan_tokenizer(corpus[7623])
scan_tokenizer(as.character(corpus[7623]))
corpus <- tm_map(x=Corpus(DataframeSource(tail(skills,-1))), FUN=removeWords, stopwords("english"))
install.packages("tm")
install.packages("RWeka")
install.packages("wordcloud")
source('~/git/opencpu/neo4jr/R/do.R')
corpus <- tm_map(x=Corpus(DataframeSource(tail(skills,-1))), FUN=removeWords, stopwords("english"))
library(tm)
corpus <- tm_map(x=Corpus(DataframeSource(tail(skills,-1))), FUN=removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
source('~/git/opencpu/neo4jr/R/do.R')
source('~/git/opencpu/neo4jr/R/do.R')
cloudMapFromSkillsets()
library(twitteR)
mach_tweets = searchTwitter("machine learning", n=500, lang="en")
source('~/git/opencpu/neo4jr/R/do.R')
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = scan_tokenizer))
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = NGramTokenizer))
?scan_tokenizer
scan_tokenizer(corpus[1])
scan_tokenizer(corpus[[1]])
NGramTokenizer(corpus[[1]])
NGramTokenizer(corpus[1])
?NGramTokenizer
NGramTokenizer(as.character(corpus[[1]]))
corpus
corpus[[1]]
corpus[[2]]
corpus[[3]]
class(corpus[[1]])
?NGramTokenizer
as.character(corpus[[1]])
NGramTokenizer(as.character(corpus[[1]]))
NGramTokenizer("Sales Marketing")
NGramTokenizer("Sales Marketing", control=NULL)
NGramTokenizer("Sales Marketing helwljfkd")
NGramTokenizer("Sales Marketing ")
BigramTokenizer("Sales Marketing ")
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
BigramTokenizer(corpus[[1]])
BigramTokenizer(corpus[[2]])
BigramTokenizer(corpus[[3]])
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
termFreq(corpus[[1]],control=list(tokenize = scan_tokenizer))
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
head(parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer)))
BigramTokenizer(corpus[[3]])
scan_tokenizer(corpus[[3]])
corpus[[1]]
class(corpus)
class(corpus[[1]])
BigramTokenizer(corpus[[1]])
scan_tokenizer(corpus[[1]])
class(BigramTokenizer(corpus[[1]]))
class(scan_tokenizer(corpus[[1]]))
termFreq(corpus[[1]],control=list(tokenize = scan_tokenizer))
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
head(parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer)))
head(parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = scan_tokenizer)))
corpus[[1]]
corpus[[2]]
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = scan_tokenizer)))[[1]]
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = scan_tokenizer))[[1]]
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = scan_tokenizer))[[2]]
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))[[1]]
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))[[2]]
debug(termFreq)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
debug(termFreq)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
debug(BigramTokenizer)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
Q
debug parallel::mclapply
debug(parallel::mclapply)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
Q
undebug(parallel::mclapply)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
traceback()
debug(parallel::mclapply)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
debug(termFreq)
c
debug(termFreq)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
FUN
debug(FUN)
c
?setBreakpoint
source('~/git/opencpu/neo4jr/R/do.R')
runRegression(1000)
source('~/git/opencpu/classification/R/do.R')
runRegression(1000)
system.time(runRegression(-1))
system.time(runRegression(50000))
system.time(result <- runRegression(100000))
system.time(result <- runRegression(200000))
nrow(result)
head(result)
head(result[order(result$score, decreasing=T)], 10)
head(result[order(result$score, decreasing=T),])
head(result[order(result$score, decreasing=T),], 40)
write.table(result[order(result$score, decreasing=T),], file="~/Downloads/PoI.csv", row.names=F, sep=",", append=F)
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
Q
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
traceback()
Q
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
Q
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = BigramTokenizer))
where
w
where
mc.allow.recursive
isChild()
debug(termFreq)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = scan_tokenizer))
Q
undebug(parallel::mclapply)
parallel::mclapply(corpus, FUN=termFreq, control = list(tokenize = scan_tokenizer))
parallel::mclapply(corpus, FUN=scan_tokenizer)
parallel::mclapply(corpus, FUN=BigramTokenizer)
parallel::mclapply(corpus, FUN=NGramTokenizer)
NGramTokenizer(corpus[[1]])
BigramTokenizer(corpus[[1]])
Q
undebug(BigramTokenizer)
BigramTokenizer(corpus[[1]])
lapply(corpus, FUN=NGramTokenizer)
lapply(corpus, FUN=BigramTokenizerTokenizer)
lapply(corpus, FUN=BigramTokenizer)
data("crude")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
head(tdm)
head(tdm, 20)
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
Q
undebug(termFreq)
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
corpus
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
lapply(corpus, FUN=BigramTokenizer)
lapply(corpus, FUN=scan_tokenizer)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
tdm
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = scan_tokenizer))
TermDocumentMatrix
debug(TermDocumentMatrix)
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = scan_tokenizer))
Q
?parallel:mclapply
?parallel::mclapply
?TermFrequencyMatrix
?TermDocumentMatrix
termFreq(corpus[[1]],control=list(tokenize = BigramTokenizer))
crude[[1]]
termFreq(crude[[1]],control=list(tokenize = BigramTokenizer))
class(crude)
class(corpus)
class(crude[[1]])
head(corpus)
head(crude)
crude
corpus
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(max = 2))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
Q
undebug(TermDocumentMatrix)
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
TermDocumentMatrix
TermDocumentMatrix.VCorpus
TermDocumentMatrix:VCorpus
TermDocumentMatrix::VCorpus
debug(TermDocumentMatrix)
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
source('~/git/opencpu/neo4jr/R/do.R')
Q
tdm <- TermDocumentMatrix2(corpus, control = list(tokenize = BigramTokenizer))
source('~/git/opencpu/neo4jr/R/do.R')
tdm <- TermDocumentMatrix2(corpus, control = list(tokenize = BigramTokenizer))
source('~/git/opencpu/neo4jr/R/do.R')
tdm <- TermDocumentMatrix2(corpus, control = list(tokenize = BigramTokenizer))
?simple_triplet_matrix
install.packages("slam")
?simple_triplet_matrix
library(slam)
?simple_triplet_matrix
source('~/git/opencpu/neo4jr/R/do.R')
tdm <- TermDocumentMatrix2(corpus, control = list(tokenize = BigramTokenizer))
?meta
install.packages("meta")
library(meta)
tdm <- TermDocumentMatrix2(corpus, control = list(tokenize = BigramTokenizer))
?meta
library(tm)
tdm <- TermDocumentMatrix2(corpus, control = list(tokenize = BigramTokenizer))
library("RWeka")
tdm <- TermDocumentMatrix2(corpus, control = list(tokenize = BigramTokenizer))
source('~/git/opencpu/neo4jr/R/do.R')
tdm <- TermDocumentMatrix2(corpus, control = list(tokenize = BigramTokenizer))
tdm
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
?TermDocumentMatrix
inspect(tdm[155:160,1:5])
tdm
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = scan_tokenizer))
inspect(tdm[155:160,1:5])
tdm
source('~/git/opencpu/neo4jr/R/do.R')
cloudMapFromSkillsets
cloudMapFromSkillsets()
library(shiny)
runApp("~/git/opencpu/neo4jr/R")
runApp("~/git/opencpu/neo4jr/R")
source('~/git/opencpu/neo4jr/R/do.R')
runApp("~/git/opencpu/neo4jr/R")
cloudMapFromSkillsets()
source('~/git/opencpu/neo4jr/R/do.R')
cloudMapFromSkillsets()
?scan_tokenizer
marketskills <- queryCypher2("match m:Market-[:HAS_MARKET]-f:EmploymentFirm-[:HAS_EMPLOYMENT_FIRM]-j-[:HAS_EMPLOYMENT]-p-[:HAS_SKILL]-s where m.display_name! <> \"\" return m.display_name!, s.display_name!")
nrow(marketskills)
?aggregate
names(marketskills)
head(marketskills)
names(marketskills) < c("market", "skill")
names(marketskills) <- c("market", "skill")
aggregate(marketskills, by=list(marketskills$market))
?tapply
?with
by(data=marketskills, INDICES=marketskills$market)
by(data=marketskills, INDICES=marketskills$market, summary)
by(data=marketskills, INDICES=marketskills$market, data.frame)
skillsByMarket <- by(data=marketskills, INDICES=marketskills$market, data.frame)
class(skillsByMarket)
skillsByMarket[[1]]
skillsByMarket[[2]]
skillsByMarket[[4]]
skillsByMarket[[6]]
skillsByMarket[[10]]
unique(marketskills$market)
skillsByMarket[["3d"]]
skillsByMarket[grepl("3d", skillsByMarket)]
skillsByMarket[[grepl("3d", skillsByMarket)]]
skillsByMarket[[grepl("3d", names(skillsByMarket)]]
names(skillsByMarket)
skillsByMarket[[grepl("3d", names(skillsByMarket))]]
grepl("3d", names(skillsByMarket))
grep("3d", names(skillsByMarket))
skillsByMarket[[grep("3d", names(skillsByMarket))]]
skillsByMarket[grep("3d", names(skillsByMarket))]
skillsByMarket[grep("3d", names(skillsByMarket))]
rbind(skillsByMarket[grep("3d", names(skillsByMarket))])
class(skillsByMarket[grep("3d", names(skillsByMarket))])
skillsByMarket[grep("3d", names(skillsByMarket))]
skillsByMarket[grep("3d", names(skillsByMarket))][[1]]
oneMarket <- skillsByMarket[grep(market(), names(skillsByMarket))][[1]]
oneMarket <- skillsByMarket[grep("3d", names(skillsByMarket))][[1]]
corpus <- tm_map(x=Corpus(DataframeSource(oneMarket, FUN=removeWords, stopwords("english"))
)
)
corpus <- tm_map(x=Corpus(DataframeSource(oneMarket, FUN=removeWords, stopwords("english"))))
corpus <- tm_map(x=Corpus(DataframeSource(oneMarket), FUN=removeWords, stopwords("english")))
DataframeSource(oneMarket)
Corpus(DataframeSource(oneMarket))
corpus <- tm_map(x=Corpus(DataframeSource(oneMarket, FUN=removeWords, stopwords("english"))))
corpus <- tm_map(x=Corpus(DataframeSource(oneMarket)), FUN=removeWords, stopwords("english"))
corpus
corpus
tdm <- TermDocumentMatrix(corpus)
?wordcloud
runApp("~/git/opencpu/neo4jr/R/skills/")
marketskills <- queryCypher2("match m:Market-[:HAS_MARKET]-f:EmploymentFirm-[:HAS_EMPLOYMENT_FIRM]-j-[:HAS_EMPLOYMENT]-p-[:HAS_SKILL]-s where m.display_name! <> \"\" return m.display_name!, s.display_name!")
marketskills
skillsByMarket
skillsByMarket <- by(data=marketskills, INDICES=marketskills$market, data.frame)
runApp("~/git/opencpu/neo4jr/R/skills/")
runApp("~/git/opencpu/neo4jr/R/skills/")
marketskills <- queryCypher2("match m:Market-[:HAS_MARKET]-f:EmploymentFirm-[:HAS_EMPLOYMENT_FIRM]-j-[:HAS_EMPLOYMENT]-p-[:HAS_SKILL]-s where m.display_name! <> \"\" return m.display_name!, s.display_name! limit 1000")
names(marketskills) <- c("market", "skill")
skillsByMarket <- by(data=marketskills, INDICES=marketskills$market, data.frame)
markets <- unique(marketskills$market)
oneMarket <- skillsByMarket[grep(market(), names(skillsByMarket))][[1]]
oneMarket <- skillsByMarket[grep("3d", names(skillsByMarket))][[1]]
oneMarket <- skillsByMarket[grep("3d", names(skillsByMarket))]
oneMarket
skillsByMarket[grep("3d", names(skillsByMarket))]
markskills
marketskills
head(marketskills)
skillsByMarket <- by(data=marketskills, INDICES=marketskills$market, data.frame)
skillsByMarket
skillsByMarket[[1]]
runApp("~/git/opencpu/neo4jr/R/skills/")
debug(runApp)
runApp("~/git/opencpu/neo4jr/R/skills/")
Q
Q
Q
undebug(runApp)
runApp("~/git/opencpu/neo4jr/R/skills/")
runApp("~/git/opencpu/neo4jr/R/skills/")
runApp("~/git/opencpu/neo4jr/R/skills/")
runApp("~/git/opencpu/neo4jr/R/skills/")
runApp("~/git/opencpu/neo4jr/R/skills/")
runApp("~/git/opencpu/neo4jr/R/skills/")
runApp("~/git/opencpu/neo4jr/R/skills/")
